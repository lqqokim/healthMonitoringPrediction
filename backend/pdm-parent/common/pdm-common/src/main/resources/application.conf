pdm = {

  # Configuration for the Kafka input topic
  input-topic = {

    # Comma-separated list of Kafka brokers, as host1:port1(,host2:port2,...)
    broker = "192.168.7.228:29092,192.168.7.228:39092,192.168.7.228:49092"
    schema-registry-url = "http://192.168.7.228:8081"

    lock = {
      # Comma-separated list of Zookeeper masters, as host1:port1(,host2:port2,...)
      # Note that if you need to specify a chroot, then weirdly, it should only appear
      # at the end: host1:port1,host2:port2/chroot, not host1:port1/chroot,host2:port2/chroot
      # The chroot is commonly "/kafka", like on CDH clusters.
      master = "localhost:2181"
    }

    message = {
      # Input topic
      topic = "pdm-input"

      # Key/message classes that the framework receives, respectively
      key-class = "java.lang.String"
      message-class = "java.lang.String"

      # Decoder classes used to read/write key/value classes
      key-decoder-class = "org.apache.kafka.common.serialization.StringDeserializer"
      message-decoder-class = "org.apache.kafka.common.serialization.KafkaAvroDeserializer"
    }

  }

  # Configuration for the Kafka sink topic
  output-topic = {

    # Comma-separated list of Kafka brokers, as host1:port1(,host2:port2,...)
    broker = "192.168.7.228:29092,192.168.7.228:39092,192.168.7.228:49092"
    schema-registry-url = "http://192.168.7.228:8081"

    lock = {
      # Comma-separated list of Zookeeper masters, as host1:port1(,host2:port2,...)
      # Note that if you need to specify a chroot, then weirdly, it should only appear
      # at the end: host1:port1,host2:port2/chroot, not host1:port1/chroot,host2:port2/chroot
      # The chroot is commonly "/kafka", like on CDH clusters.
      master = "localhost:2181"
    }

    message = {
      # Input topic
      topic = "pdm-output"

      # Key/message classes that the framework receives, respectively
      key-class = "java.lang.String"
      message-class = "java.lang.String"

      # Decoder classes used to read/write key/value classes
      key-decoder-class = "org.apache.kafka.common.serialization.ByteArraySerializer"
      message-decoder-class = "org.apache.kafka.common.serialization.ByteArraySerializer"
    }
  }

  # Batch layer configuration
  batch = {

    # Streaming framework configuration
    streaming = {

      bootstrap-servers = "192.168.7.228:29092,192.168.7.228:39092,192.168.7.228:49092"
      source-topic-name = "pdm-input"
      output-topic-name = "pdm-output"

    }

  }

  # Speed layer configuration
  speed = {

  }

  # Serving layer configuration
  serving = {

    #Configuring the HikariCP Pools
    repository = {
      poolName = pdmserving
      jdbcUrl = "jdbc:oracle:thin:@//192.168.8.36:1521/pdm"
      username = npdm
      password = bistel01
      maximumPoolSize = 50
      minimumIdle = 10
      cachePrepStmts = true
      prepStmtCacheSize = 256
      prepStmtCacheSqlLimit = 2048
      useServerPrepStmts = true
    }

  }

  #producer configuration
  producer = {

    #kafka client
    kafka = {
      clientId = "DEFAULT"
      bootstrapServers = "192.168.7.228:29092,192.168.7.228:39092,192.168.7.228:49092"
      acks = "all"
      retries = 0
      batchSize = 16384
      linger-ms = 1
      key-serializer = "org.apache.kafka.common.serialization.StringSerializer"
      value-serializer = "org.apache.kafka.common.serialization.ByteArraySerializer"
      schema-registry-url = "http://192.168.7.228:8081"
    }

    #Configuring the HikariCP Pools
    source = {
      poolName = pdmproducer
      jdbcUrl = "jdbc:oracle:thin:@//192.168.8.36:1521/pdm"
      username = npdm
      password = bistel01
      maximumPoolSize = 50
      minimumIdle = 10
      cachePrepStmts = true
      prepStmtCacheSize = 256
      prepStmtCacheSqlLimit = 2048
      useServerPrepStmts = true
    }

    master = {
      servingLayerServer = "http://192.168.7.227:28000"
    }

  }

  # consumer configuration
  consumer = {

    #Configuring the HikariCP Pools
    repository = {
      poolName = pdmconsumer
      jdbcUrl = "jdbc:oracle:thin:@//192.168.8.36:1521/pdm"
      username = npdm
      password = bistel01
      maximumPoolSize = 50
      minimumIdle = 10
      cachePrepStmts = true
      prepStmtCacheSize = 256
      prepStmtCacheSqlLimit = 2048
      useServerPrepStmts = true
    }

    #Configuration
    kafka = {
      groupId = "DEFAULT"
      bootstrapServers = "192.168.7.228:29092,192.168.7.228:39092,192.168.7.228:49092"
      auto-commit-enable = false
      auto-offset-reset = "earliest"
      specific-avro-reader = true
      key-deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
      value-deserializer = "org.apache.kafka.common.serialization.ByteArrayDeserializer"
      schema-registry-url = "http://192.168.7.228:8081"
    }

  }

  scheduler = {
    path = "./conf/quartz.properties"
  }

}
