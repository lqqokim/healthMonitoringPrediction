# kafka property

# Deserializer class for key that implements the org.apache.kafka.common.serialization.Deserializer interface.
key.deserializer=org.apache.kafka.common.serialization.StringDeserializer

# Deserializer class for value that implements the org.apache.kafka.common.serialization.Deserializer interface.
value.deserializer=org.apache.kafka.common.serialization.ByteArrayDeserializer

# A unique string that identifies the consumer group this consumer belongs to. This property is
# required if the consumer uses either the group management functionality by using subscribe(topic)
# or the Kafka-based offset management strategy.
group.id=pdm-default-store

# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
# The client will make use of all servers irrespective of which servers are specified here for
# bootstrappingâ€”this list only impacts the initial hosts used to discover the full set of servers.
# This list should be in the form host1:port1,host2:port2,.... Since these servers are just used
# for the initial connection to discover the full cluster membership (which may change dynamically),
# this list need not contain the full set of servers (you may want more than one, though, in case a server is down).
bootstrap.servers=192.168.7.228:29092,192.168.7.228:39092,192.168.7.228:49092

# What to do when there is no initial offset in Kafka or if the current offset does not exist any more
# on the server (e.g. because that data has been deleted):
#   earliest: automatically reset the offset to the earliest offset
#   latest: automatically reset the offset to the latest offset
#   none: throw exception to the consumer if no previous offset is found for the consumer's group
#   anything else: throw exception to the consumer.
auto.offset.reset=latest

# If true the consumer's offset will be periodically committed in the background.
enable.auto.commit=false

# Close idle connections after the number of milliseconds specified by this config.
connections.max.idle.ms=540000

# The minimum amount of data the server should return for a fetch request.
# If insufficient data is available the request will wait for that much data to accumulate before answering the request.
# The default setting of 1 byte means that fetch requests are answered as soon as a single byte of data is available
# or the fetch request times out waiting for data to arrive. Setting this to something greater than 1 will cause
# the server to wait for larger amounts of data to accumulate which can improve server throughput a bit at the cost
# of some additional latency.
fetch.min.bytes=1

# The expected time between heartbeats to the consumer coordinator when using Kafka's group management facilities.
# Heartbeats are used to ensure that the consumer's session stays active and to facilitate rebalancing when
# new consumers join or leave the group. The value must be set lower than session.timeout.ms, but typically
# should be set no higher than 1/3 of that value. It can be adjusted even lower to control the expected time
# for normal rebalances.
heartbeat.interval.ms=3000

# The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer.
# If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch
# will still be returned to ensure that the consumer can make progress. The maximum record batch size accepted
# by the broker is defined via message.max.bytes (broker config) or max.message.bytes (topic config).
# See fetch.max.bytes for limiting the consumer request size.
max.partition.fetch.bytes=1048576

# The timeout used to detect consumer failures when using Kafka's group management facility.
# The consumer sends periodic heartbeats to indicate its liveness to the broker. If no heartbeats are received
# by the broker before the expiration of this session timeout, then the broker will remove this consumer from
# the group and initiate a rebalance. Note that the value must be in the allowable range as configured in the broker
# configuration by group.min.session.timeout.ms and group.max.session.timeout.ms.
session.timeout.ms=10000

# Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.
#security.protocol=SASL_SSL

# SASL mechanism used for client connections. This may be any mechanism for which a security provider
# is available. GSSAPI is the default mechanism.
#sasl.mechanism=PLAIN

# The SSL protocol used to generate the SSLContext. Default setting is TLS, which is fine for most cases.
# Allowed values in recent JVMs are TLS, TLSv1.1 and TLSv1.2. SSL, SSLv2 and SSLv3 may be supported
# in older JVMs, but their usage is discouraged due to known security vulnerabilities.
#ssl.protocol=TLSv1.2

# The list of protocols enabled for SSL connections.
#ssl.enabled.protocols=TLSv1.2

# The location of the trust store file.
#ssl.truststore.location=

# The password for the trust store file. If a password is not set access to the truststore is still
# available, but integrity checking is disabled.
#ssl.truststore.password=changeit

# The file format of the trust store file.
#ssl.truststore.type=JKS

# The endpoint identification algorithm to validate server hostname using server certificate.
#ssl.endpoint.identification.algorithm=HTTPS

# JAAS login context parameters for SASL connections in the format used by JAAS configuration files.
# JAAS configuration file format is described here. The format for the value is: ' (=)*;'
#sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="USERNAME" password="PASSWORD";